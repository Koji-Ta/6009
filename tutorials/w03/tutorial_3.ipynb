{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda Internet Radio\n",
    "\n",
    "Goal: Implementing the Panda `next_song` function based on history of song likes and dislikes.\n",
    "\n",
    "Side Effect: Learning about `docstrings` and `doctests`\n",
    "\n",
    "_Note: This tutorial implements a slightly different spec than defined by the panda.ipyb readme and tested by test.py_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Function\n",
    "\n",
    "A song is represented as a list of genes, it's \"genome\". Each gene can have value either 0 or 1.\n",
    "\n",
    "We want a `distance` function that will give us a sense of how different two songs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(song_0, song_1):\n",
    "    \"\"\"Get distance between two songs\n",
    "    \n",
    "    inputs: Two songs, defined by their genome.\n",
    "    returns: The \"mangatten distance\" between two genomes,\n",
    "        i.e., the number of genes deffering between songs.\n",
    "    >>> song_0 = []\n",
    "    >>> song_1 = []\n",
    "    >>> distance(song_0, song_1)\n",
    "    0\n",
    "    >>> song_0 = [0]\n",
    "    >>> song_1 = [1]\n",
    "    >>> distance(song_0, song_1)\n",
    "    1\n",
    "    >>> distance(song_1, song_0)\n",
    "    1\n",
    "    >>> distance([0, 1], [1, 0])\n",
    "    2\n",
    "    >>> distance([0], [1, 1])\n",
    "    Traceback (most recent call last):\n",
    "     ...\n",
    "    ValueError: song genomes different length\n",
    "    \"\"\"\n",
    "    if len(song_0) != len(song_1):\n",
    "        raise ValueError('song genomes different length')\n",
    "    return sum(abs(gene_0 - gene_1) for gene_0, gene_1 in zip(song_0, song_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance([0, 1], [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(distance, globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docstrings and doctests\n",
    "\n",
    "**`doctests`** are test cases embedded within `docstrings`, that can actually be run and tested automatically:\n",
    "* Nothing output if all tests succeed\n",
    "* An error reported if one or more tests fail\n",
    "\n",
    "These are part of the standard python library, so are always available. Read more about them at \n",
    "<a href=\"https://docs.python.org/3/library/doctest.html\">https://docs.python.org/3/library/doctest.html</a>\n",
    "\n",
    "A useful approach is to include the following at the bottom of your file (e.g., your `lab.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # running lab.py invokes the doctests for *all* functions in the file...\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside jupyter, we'll instead invoke specific doctests directly, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(distance, globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Distance\n",
    "\n",
    "Add another data structure -- a \"music library\" implemented as a dictionary consisting of song_ids as keys, and the corresponding genome for the song as the value.\n",
    "\n",
    "Now we'd like to get a sense of the average distance between one song and a whole list of other songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distance(song_id_list, song_id, music):\n",
    "    \"\"\"Return average distance between song_id and music library\n",
    "    \n",
    "    inputs: list of song_ids, a single song_id, and a music dictionary\n",
    "    returns: average distance, computed as the sum of distances\n",
    "      divided by the number of distances considered, between song given\n",
    "      by song_id and the songs in song_id_list\n",
    "    note: average_distance from empty list is 0\n",
    "    >>> music = {'Stairway': [0,0],\n",
    "    ...          '5th': [0,1],\n",
    "    ...          'Blues': [1,1],\n",
    "    ...          'Requiem': [1,0]}\n",
    "    >>> average_distance([], 'Stairway', music)\n",
    "    0.0\n",
    "    >>> average_distance(['Stairway'], 'Stairway', music)\n",
    "    0.0\n",
    "    >>> average_distance(['5th'], 'Stairway', music)\n",
    "    1.0\n",
    "    >>> average_distance(['5th','Blues'], 'Stairway', music)\n",
    "    1.5\n",
    "    >>> average_distance(['5th','Blues','Requiem'], 'Stairway', music)\n",
    "    1.3333333333333333\n",
    "    \"\"\"\n",
    "    dist = sum(distance(music[song_id], music[other_id])\n",
    "                 for other_id in song_id_list)\n",
    "    return dist / max(1, len(song_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(average_distance, globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that doctest using string comparisons between the expected and actual output, not more sophisticated tests like ==. Thus in the above, we need 0.0 for  expected return values, not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness Function\n",
    "\n",
    "The \"goodness\" of a song is defined to be the average distance of the song from a list of disliked songs, minus the average distance of the song from a list of liked songs. This is meant to favor songs far away from disliked songs, \n",
    "      but close to liked songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodness(likes, dislikes, song_id, music):\n",
    "    \"\"\"Return `goodness` of song_id based on history of likes/dislikes\n",
    "    \n",
    "    inputs: likes, dislikes are lists of 'liked' and 'disliked' song_ids.\n",
    "            song_id is the id of a song we'd like to know the \"goodness\" of.\n",
    "            music is a music dictionary.\n",
    "    returns: \"goodness\" value (float) of song_id\n",
    "    >>> music = {'Stairway': [0,0],\n",
    "    ...          '5th': [0,1],\n",
    "    ...          'Blues': [1,1],\n",
    "    ...          'Requiem': [1,0]}\n",
    "    >>> likes = []\n",
    "    >>> dislikes = []\n",
    "    >>> goodness(likes, dislikes, 'Stairway', music)\n",
    "    0.0\n",
    "    >>> likes = ['Requiem']\n",
    "    >>> dislikes = ['5th', 'Blues']\n",
    "    >>> goodness(likes, dislikes, 'Stairway', music)\n",
    "    0.5\n",
    "    \"\"\"\n",
    "    return average_distance(dislikes, song_id, music) - \\\n",
    "           average_distance(likes, song_id, music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(goodness, globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Song\n",
    "\n",
    "Now to answer the key question -- what song should be picked next, based on previously played song likes and dislikes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_song(likes, dislikes, music):\n",
    "    \"\"\"Return next song to play based on history of likes/dislikes\n",
    "    \n",
    "    inputs: likes is list of 'liked' previously played song ids.\n",
    "            dislikes is list of 'disliked' previously played song ids.\n",
    "            music is a music dictionary.\n",
    "    returns: ID for an unplayed song in dictionary with best goodness value\n",
    "\n",
    "    >>> music = {'Stairway': [0,0],\n",
    "    ...          '5th': [0,1],\n",
    "    ...          'Blues': [1,1],\n",
    "    ...          'Requiem': [1,0]}\n",
    "    >>> likes = []\n",
    "    >>> dislikes = ['Blues']\n",
    "    >>> next_song(likes, dislikes, music)\n",
    "    'Stairway'\n",
    "\n",
    "    >>> likes = ['Blues']\n",
    "    >>> dislikes = []\n",
    "    >>> nxt = next_song(likes, dislikes, music)\n",
    "    >>> nxt == '5th' or nxt == 'Requiem'\n",
    "    True\n",
    "    \"\"\"\n",
    "    not_played = set(music.keys()) - set(likes) - set(dislikes)\n",
    "    next_song = max((goodness(likes, dislikes, song_id, music), song_id)\n",
    "                 for song_id in not_played)[1]\n",
    "    return next_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(next_song, globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_song (best goodness) is song id: Requiem\n",
      "goodness for song id Stairway = -2.0\n",
      "goodness for song id 5th = -1.0\n",
      "goodness for song id Blues = 0.0\n",
      "goodness for song id Requiem = -1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's debug this case a little more...\n",
    "music = {'Stairway': [0,0],\n",
    "         '5th': [0,1],\n",
    "         'Blues': [1,1],\n",
    "         'Requiem': [1,0]}\n",
    "likes = ['Blues']\n",
    "dislikes = []\n",
    "print('next_song (best goodness) is song id:', \n",
    "      next_song(likes, dislikes, music))\n",
    "for song_id in music:\n",
    "    print('goodness for song id', song_id, \"=\", \n",
    "          goodness(likes, dislikes, song_id, music))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we only had one of the possible return values in our test case. Maybe we should be more thorough. Might the following work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Requiem'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes = ['Blues']\n",
    "dislikes = []\n",
    "next_song(likes, dislikes, music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really. So how deal with that ambiguous return value, using doctest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Using Python random module to generate large test cases\n",
    "    \n",
    "(Note -- in 6.009 we're not allowing `import random`; but this gives you the idea)\n",
    "\n",
    "How do you check your output is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test(music_size, gene_length, like_size, dislike_size):\n",
    "    random.seed(6009) # A fixed random seed results in deterministic output.\n",
    "    music = {}\n",
    "    for i in range(music_size):\n",
    "        music[\"song_\"+str(i)] = [random.randint(0,1) for _ in range(gene_length)]\n",
    "    likes = random.sample(music.keys(), like_size)\n",
    "    dislikes = random.sample(music.keys(), dislike_size)\n",
    "    return music, likes, dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music: {'song_0': [0, 1, 0, 0, 1, 1, 0, 0, 1, 0], 'song_1': [1, 1, 0, 1, 1, 0, 1, 0, 1, 1], 'song_2': [1, 0, 1, 0, 1, 1, 0, 1, 0, 0], 'song_3': [0, 0, 0, 1, 1, 0, 1, 1, 0, 0], 'song_4': [0, 1, 0, 1, 1, 0, 1, 0, 1, 0], 'song_5': [1, 0, 1, 0, 0, 0, 1, 0, 1, 1], 'song_6': [0, 0, 0, 1, 0, 0, 0, 1, 1, 1], 'song_7': [1, 0, 0, 1, 0, 1, 0, 1, 1, 0], 'song_8': [0, 0, 0, 1, 1, 0, 1, 1, 1, 1], 'song_9': [1, 0, 1, 1, 1, 0, 0, 0, 0, 1], 'song_10': [0, 0, 0, 0, 1, 1, 1, 0, 1, 0], 'song_11': [0, 1, 1, 1, 0, 1, 1, 0, 0, 0], 'song_12': [0, 0, 1, 0, 0, 1, 1, 1, 1, 1], 'song_13': [1, 1, 0, 0, 0, 1, 1, 0, 0, 1], 'song_14': [0, 0, 0, 1, 0, 1, 1, 0, 0, 0], 'song_15': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'song_16': [0, 1, 0, 0, 1, 0, 1, 1, 0, 0], 'song_17': [0, 1, 1, 0, 1, 0, 0, 0, 0, 0], 'song_18': [0, 1, 1, 1, 1, 1, 0, 0, 1, 0], 'song_19': [1, 0, 1, 1, 1, 0, 0, 1, 0, 1], 'song_20': [0, 1, 0, 0, 0, 0, 1, 1, 1, 1], 'song_21': [1, 1, 0, 0, 1, 0, 0, 0, 0, 0], 'song_22': [0, 0, 1, 1, 0, 0, 1, 1, 0, 0], 'song_23': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'song_24': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'song_25': [0, 0, 0, 1, 0, 1, 1, 1, 1, 1], 'song_26': [1, 1, 0, 0, 1, 0, 0, 0, 1, 0], 'song_27': [1, 1, 1, 1, 1, 1, 1, 1, 0, 1], 'song_28': [1, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'song_29': [0, 1, 1, 1, 0, 1, 1, 1, 1, 0]}\n",
      "Example: song_1 = [1, 1, 0, 1, 1, 0, 1, 0, 1, 1]\n",
      "next_song_id: song_4\n",
      "goodness: -4.1\n"
     ]
    }
   ],
   "source": [
    "music, likes, dislikes = generate_test(30, 10, 10, 0)\n",
    "print(\"music:\", music)\n",
    "print(\"Example: song_1 =\", music[\"song_1\"])\n",
    "\n",
    "next_song_id = next_song(likes, dislikes, music)\n",
    "print(\"next_song_id:\", next_song_id)\n",
    "print(\"goodness:\", goodness(likes, dislikes, next_song_id, music))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if the next_song_id is reasonable or correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the best goodness actually is, among unplayed songs...\n",
    "goodness_values = [goodness(likes, dislikes, song, music)\n",
    "                       for song in music\n",
    "                           if song not in likes and song not in dislikes]\n",
    "max(goodness_values)\n",
    "\n",
    "#or as a comprehension without building the list...\n",
    "max(goodness(likes, dislikes, song, music)\n",
    "        for song in music\n",
    "            if song not in likes and song not in dislikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that now we're <b>writing more complicated code to verify</b> or check answers. That's a job most likely better suited to `unittest`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
